{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('3.8')"
  },
  "interpreter": {
   "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf \n",
    "import csv \n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import mitdeeplearning as mdl\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten\n",
    "\n",
    "\n",
    "import datetime"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%load_ext tensorboard"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rm -rf ./logs/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def file_select():  \n",
    "    global accelometer\n",
    "    global gyroscope\n",
    "    global output_data\n",
    "    global speed\n",
    "    global df\n",
    "\n",
    "    num = random.randint(3,9)\n",
    "    df = pd.read_csv('/Users/amitaflalo/Desktop/deepnav/data/train/locationData ' + str(num) + '.csv', header= None)\n",
    "    accelometer = df.iloc[:, 2:5] \n",
    "    gyroscope = df.iloc[:, 5:8] \n",
    "    output_data = df.iloc[:, 9:10] \n",
    "    speed = df.iloc[:, 9:10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def file_select_test():  \n",
    "    global accelometer\n",
    "    global gyroscope\n",
    "    global output_data\n",
    "    global speed\n",
    "    global df\n",
    "\n",
    "    df = pd.read_csv('/Users/amitaflalo/Desktop/deepnav/data/test/locationData'  + '.csv', header= None)\n",
    "    accelometer = df.iloc[:, 2:5] \n",
    "    gyroscope = df.iloc[:, 5:8] \n",
    "    output_data = df.iloc[:, 9:10] \n",
    "    speed = df.iloc[:, 9:10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_batch(accelometer, gyroscope, output_data, speed, batch_size):\n",
    "    \n",
    "    idx = random.randint(0, ((len(df) - (batch_size + 1) * 10) // 10))\n",
    "\n",
    "    #input to Network\n",
    "    ##########################################################################################\n",
    "\n",
    "    input_batch_accelometer = [accelometer.iloc[i * 10 : i * 10 +10,:] for i in range(idx,idx + batch_size)]\n",
    "    input_batch_gyroscope = [gyroscope.iloc[i * 10 : i * 10 +10,:] for i in range(idx,idx + batch_size)]\n",
    "\n",
    "    a = np.array(input_batch_accelometer)\n",
    "    b = np.array(input_batch_gyroscope)\n",
    "    \n",
    "    x_out = np.concatenate([a,b], axis= 2)\n",
    "\n",
    "    #ground truth[azimut diff, acceleraion(m/s)]\n",
    "    ########################################################################################## \n",
    "\n",
    "    \n",
    "    out = [output_data.iloc[i * 10,:] for i in range(idx + 1,idx + batch_size + 1)]\n",
    "    y = np.array(out, dtype='float64')\n",
    "\n",
    "    x_out = np.expand_dims(x_out, axis = 3)\n",
    "\n",
    "    return x_out, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x, y = get_batch(accelometer, gyroscope, output_data, speed, 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x.shape\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y = Conv2D(16, (1,3), strides=(1,3), activation='relu', input_shape = (10,6,1))(x)\n",
    "y = Conv2D(32, (2,2), strides=(1,1), activation='relu')(y)\n",
    "\n",
    "# y = MaxPool2D(pool_size=(2,2))(y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(y.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def build_model():\n",
    "  model = tf.keras.Sequential([\n",
    "    Conv2D(16, (1,3), strides=(1,3), activation='relu', input_shape = (10,6,1)),\n",
    "    Conv2D(32, (2,2), strides=(1,1), activation='relu'),\n",
    "    Conv2D(32, (2,1), strides=(2,1), activation='relu'),\n",
    "    Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  return model\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def compute_loss(labels, logits):\n",
    "  mse = tf.keras.losses.MeanAbsoluteError()\n",
    "  loss = mse(labels, logits)\n",
    "  return loss\n",
    "# mse = tf.keras.losses.MeanSquareError()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def precent(labels, logits):\n",
    "    mae = tf.keras.losses.MeanAbsolutePercentageError()\n",
    "    loss = mae(labels, logits)\n",
    "      \n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define our metrics\n",
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Hyperparameter setting and optimization ###\n",
    "\n",
    "# Optimization parameters:\n",
    "num_training_iterations= 2000  # Increase this to train longer\n",
    "batch_size = 3  # Experiment between 1 and 64\n",
    "learning_rate = 1e-3  # Experiment between 1e-5 and 1e-1\n",
    "\n",
    "# Checkpoint location: \n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "model = build_model()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y): \n",
    "  # Use tf.GradientTape()\n",
    "  with tf.GradientTape() as tape:\n",
    "      y_hat = model(x) \n",
    "      loss = compute_loss(y, y_hat)\n",
    "\n",
    "  # Now, compute the gradients \n",
    "  grads = tape.gradient(loss, model.trainable_variables) \n",
    "  # Apply the gradients to the optimizer so it can update the model accordingly\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "  \n",
    "  #metrics\n",
    "  train_loss(loss)\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y): \n",
    "    y_hat = model(x) \n",
    "    # loss = compute_loss(y, y_hat)\n",
    "    loss = precent(y, y_hat)\n",
    "    \n",
    "    test_loss(loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "##################\n",
    "# Begin training!#\n",
    "##################\n",
    "\n",
    "for iter in range(num_training_iterations):\n",
    "\n",
    "  # Grab a batch and propagate it through the network\n",
    "  file_select()\n",
    "  x_batch, y_batch = get_batch(accelometer, gyroscope,speed, output_data, batch_size)\n",
    "  train_step(x_batch, y_batch)\n",
    "\n",
    "  with train_summary_writer.as_default():\n",
    "    tf.summary.scalar('loss', train_loss.result(), step=iter)\n",
    "\n",
    "  file_select_test()\n",
    "  x_batch, y_batch = get_batch(accelometer, gyroscope,speed, output_data, batch_size)\n",
    "  loss = test_step(x_batch, y_batch)\n",
    "\n",
    "  with test_summary_writer.as_default():\n",
    "    tf.summary.scalar('loss_test', test_loss.result(), step=iter)\n",
    "\n",
    "  # Update the model with the changed weights!\n",
    "  if iter % 100 == 0:     \n",
    "    model.save_weights(checkpoint_prefix)\n",
    "\n",
    "  template = 'iter {}, Loss: {}, Test Loss: {}'\n",
    "  print (template.format(iter+1,\n",
    "                         train_loss.result(), \n",
    "                         test_loss.result())) \n",
    "\n",
    "\n",
    "  # Reset metrics every epoch\n",
    "  train_loss.reset_states()\n",
    "  test_loss.reset_states()\n",
    "    \n",
    "# Save the trained model and the weights\n",
    "model.save_weights(checkpoint_prefix)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%tensorboard --logdir /Users/amitaflalo/Desktop/deepnav/logs\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}