{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7",
   "display_name": "Python 3.8.3 64-bit ('3.8')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import csv \n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import mitdeeplearning as mdl\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten\n",
    "\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_select():  \n",
    "    global accelometer\n",
    "    global gyroscope\n",
    "    global output_data\n",
    "    global speed\n",
    "    global df\n",
    "\n",
    "    num = random.randint(3,9)\n",
    "    df = pd.read_csv('/Users/amitaflalo/Desktop/deepnav/data/train/locationData ' + str(num) + '.csv', header= None)\n",
    "    accelometer = df.iloc[:, 2:5] \n",
    "    gyroscope = df.iloc[:, 5:8] \n",
    "    output_data = df.iloc[:, 9:10] \n",
    "    speed = df.iloc[:, 9:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_select_test():  \n",
    "    global accelometer\n",
    "    global gyroscope\n",
    "    global output_data\n",
    "    global speed\n",
    "    global df\n",
    "\n",
    "    df = pd.read_csv('/Users/amitaflalo/Desktop/deepnav/data/test/locationData'  + '.csv', header= None)\n",
    "    accelometer = df.iloc[:, 2:5] \n",
    "    gyroscope = df.iloc[:, 5:8] \n",
    "    output_data = df.iloc[:, 9:10] \n",
    "    speed = df.iloc[:, 9:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(accelometer, gyroscope, output_data, speed, batch_size):\n",
    "    \n",
    "    idx = random.randint(0, ((len(df) - (batch_size + 1) * 10) // 10))\n",
    "\n",
    "    #input to Network\n",
    "    ##########################################################################################\n",
    "\n",
    "    input_batch_accelometer = [accelometer.iloc[i * 10 : i * 10 +10,:] for i in range(idx,idx + batch_size)]\n",
    "    input_batch_gyroscope = [gyroscope.iloc[i * 10 : i * 10 +10,:] for i in range(idx,idx + batch_size)]\n",
    "\n",
    "    a = np.array(input_batch_accelometer)\n",
    "    b = np.array(input_batch_gyroscope)\n",
    "    \n",
    "    x_out = np.concatenate([a,b], axis= 2)\n",
    "\n",
    "    #ground truth[azimut diff, acceleraion(m/s)]\n",
    "    ########################################################################################## \n",
    "\n",
    "    \n",
    "    out = [output_data.iloc[i * 10,:] for i in range(idx + 1,idx + batch_size + 1)]\n",
    "    y = np.array(out, dtype='float64')\n",
    "\n",
    "    x_out = np.expand_dims(x_out, axis = 3)\n",
    "\n",
    "    return x_out, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_batch(accelometer, gyroscope, output_data, speed, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 10, 6, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 391
    }
   ],
   "source": [
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Conv2D(16, (1,3), strides=(1,3), activation='relu', input_shape = (10,6,1))(x)\n",
    "y = Conv2D(32, (2,2), strides=(1,1), activation='relu')(y)\n",
    "\n",
    "# y = MaxPool2D(pool_size=(2,2))(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 9, 1, 32)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  model = tf.keras.Sequential([\n",
    "    Conv2D(16, (1,3), strides=(1,3), activation='relu', input_shape = (10,6,1)),\n",
    "    Conv2D(32, (2,2), strides=(1,1), activation='relu'),\n",
    "    Conv2D(32, (2,1), strides=(2,1), activation='relu'),\n",
    "    Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(labels, logits):\n",
    "  mse = tf.keras.losses.MeanAbsoluteError()\n",
    "  loss = mse(labels, logits)\n",
    "  return loss\n",
    "# mse = tf.keras.losses.MeanSquareError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precent(labels, logits):\n",
    "    mae = tf.keras.losses.MeanAbsolutePercentageError()\n",
    "    loss = mae(labels, logits)\n",
    "      \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our metrics\n",
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter setting and optimization ###\n",
    "\n",
    "# Optimization parameters:\n",
    "num_training_iterations= 2000  # Increase this to train longer\n",
    "batch_size = 3  # Experiment between 1 and 64\n",
    "learning_rate = 1e-3  # Experiment between 1e-5 and 1e-1\n",
    "\n",
    "# Checkpoint location: \n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = build_model()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y): \n",
    "  # Use tf.GradientTape()\n",
    "  with tf.GradientTape() as tape:\n",
    "      y_hat = model(x) \n",
    "      loss = compute_loss(y, y_hat)\n",
    "\n",
    "  # Now, compute the gradients \n",
    "  grads = tape.gradient(loss, model.trainable_variables) \n",
    "  # Apply the gradients to the optimizer so it can update the model accordingly\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "  \n",
    "  #metrics\n",
    "  train_loss(loss)\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y): \n",
    "    y_hat = model(x) \n",
    "    # loss = compute_loss(y, y_hat)\n",
    "    loss = precent(y, y_hat)\n",
    "    \n",
    "    test_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7, Test Loss: 103.97442626953125\n",
      "iter 1699, Loss: 0.6436759233474731, Test Loss: 22.13088035583496\n",
      "iter 1700, Loss: 0.5053232312202454, Test Loss: 66.53489685058594\n",
      "iter 1701, Loss: 0.12565214931964874, Test Loss: 94.4637680053711\n",
      "iter 1702, Loss: 0.5425272583961487, Test Loss: 60.1038818359375\n",
      "iter 1703, Loss: 0.7526385188102722, Test Loss: 32.712867736816406\n",
      "iter 1704, Loss: 0.09414172172546387, Test Loss: 44.25996780395508\n",
      "iter 1705, Loss: 0.6299312710762024, Test Loss: 35.78813171386719\n",
      "iter 1706, Loss: 0.4997311532497406, Test Loss: 381.4947814941406\n",
      "iter 1707, Loss: 0.16777320206165314, Test Loss: 43.22249984741211\n",
      "iter 1708, Loss: 0.8313570022583008, Test Loss: 457.0963134765625\n",
      "iter 1709, Loss: 0.5930596590042114, Test Loss: 2665.647216796875\n",
      "iter 1710, Loss: 0.4999663829803467, Test Loss: 7716.30615234375\n",
      "iter 1711, Loss: 0.4959437847137451, Test Loss: 1103.661865234375\n",
      "iter 1712, Loss: 0.6611565947532654, Test Loss: 63.22999954223633\n",
      "iter 1713, Loss: 0.9166948795318604, Test Loss: 108.00821685791016\n",
      "iter 1714, Loss: 1.2167679071426392, Test Loss: 62.840606689453125\n",
      "iter 1715, Loss: 0.6416648030281067, Test Loss: 2018.2037353515625\n",
      "iter 1716, Loss: 0.49002814292907715, Test Loss: 1746.6041259765625\n",
      "iter 1717, Loss: 0.5818495154380798, Test Loss: 126.1033935546875\n",
      "iter 1718, Loss: 0.721201479434967, Test Loss: 150.86019897460938\n",
      "iter 1719, Loss: 0.08932805061340332, Test Loss: 54.97439956665039\n",
      "iter 1720, Loss: 0.5476239919662476, Test Loss: 2028.0128173828125\n",
      "iter 1721, Loss: 0.039328429847955704, Test Loss: 67.59545135498047\n",
      "iter 1722, Loss: 0.8245717883110046, Test Loss: 557.7662353515625\n",
      "iter 1723, Loss: 0.3115006387233734, Test Loss: 21.973474502563477\n",
      "iter 1724, Loss: 0.41888427734375, Test Loss: 18.740554809570312\n",
      "iter 1725, Loss: 0.42402324080467224, Test Loss: 115.85057830810547\n",
      "iter 1726, Loss: 0.8731479048728943, Test Loss: 26.00720977783203\n",
      "iter 1727, Loss: 0.37774458527565, Test Loss: 90.67410278320312\n",
      "iter 1728, Loss: 0.2532658874988556, Test Loss: 18.104928970336914\n",
      "iter 1729, Loss: 0.6436921954154968, Test Loss: 25.363683700561523\n",
      "iter 1730, Loss: 0.2221822589635849, Test Loss: 570.6707763671875\n",
      "iter 1731, Loss: 0.3845234215259552, Test Loss: 150.99232482910156\n",
      "iter 1732, Loss: 0.053905535489320755, Test Loss: 42.38242721557617\n",
      "iter 1733, Loss: 0.16297350823879242, Test Loss: 101.78640747070312\n",
      "iter 1734, Loss: 0.428037166595459, Test Loss: 84.77706146240234\n",
      "iter 1735, Loss: 0.5636312961578369, Test Loss: 17.499217987060547\n",
      "iter 1736, Loss: 0.2904655933380127, Test Loss: 30.12773895263672\n",
      "iter 1737, Loss: 0.16840510070323944, Test Loss: 6232658.5\n",
      "iter 1738, Loss: 0.6096253991127014, Test Loss: 112.31482696533203\n",
      "iter 1739, Loss: 0.38861098885536194, Test Loss: 60.312957763671875\n",
      "iter 1740, Loss: 0.8503427505493164, Test Loss: 3.0366811752319336\n",
      "iter 1741, Loss: 0.6778635382652283, Test Loss: 168.68316650390625\n",
      "iter 1742, Loss: 0.1561134308576584, Test Loss: 93.57467651367188\n",
      "iter 1743, Loss: 0.19483178853988647, Test Loss: 58.828861236572266\n",
      "iter 1744, Loss: 1.0166324377059937, Test Loss: 886.4856567382812\n",
      "iter 1745, Loss: 0.3018229007720947, Test Loss: 122.0177993774414\n",
      "iter 1746, Loss: 0.6092543005943298, Test Loss: 57.55961227416992\n",
      "iter 1747, Loss: 0.3112955093383789, Test Loss: 80.71559143066406\n",
      "iter 1748, Loss: 0.26574429869651794, Test Loss: 1339.0748291015625\n",
      "iter 1749, Loss: 0.37227174639701843, Test Loss: 13.48592758178711\n",
      "iter 1750, Loss: 0.2197546511888504, Test Loss: 100.9210205078125\n",
      "iter 1751, Loss: 0.8765255808830261, Test Loss: 120.45586395263672\n",
      "iter 1752, Loss: 0.5950840711593628, Test Loss: 100.74181365966797\n",
      "iter 1753, Loss: 0.42443227767944336, Test Loss: 222913232.0\n",
      "iter 1754, Loss: 0.2635708153247833, Test Loss: 14.887965202331543\n",
      "iter 1755, Loss: 0.3908383846282959, Test Loss: 542.3114013671875\n",
      "iter 1756, Loss: 0.17904289066791534, Test Loss: 3658.740966796875\n",
      "iter 1757, Loss: 0.4137202203273773, Test Loss: 607.0155639648438\n",
      "iter 1758, Loss: 0.2714657485485077, Test Loss: 3724.544189453125\n",
      "iter 1759, Loss: 0.7484336495399475, Test Loss: 171.18373107910156\n",
      "iter 1760, Loss: 0.6648318767547607, Test Loss: 2093.482421875\n",
      "iter 1761, Loss: 0.16595856845378876, Test Loss: 393.1637878417969\n",
      "iter 1762, Loss: 1.2271174192428589, Test Loss: 127.8289566040039\n",
      "iter 1763, Loss: 0.22246070206165314, Test Loss: 221.5128936767578\n",
      "iter 1764, Loss: 0.6652356386184692, Test Loss: 533835552.0\n",
      "iter 1765, Loss: 0.6311113238334656, Test Loss: 261.5862731933594\n",
      "iter 1766, Loss: 0.4427392780780792, Test Loss: 124.64258575439453\n",
      "iter 1767, Loss: 0.5121739506721497, Test Loss: 66.91836547851562\n",
      "iter 1768, Loss: 0.1784178763628006, Test Loss: 30.48476219177246\n",
      "iter 1769, Loss: 0.2803643047809601, Test Loss: 16.442916870117188\n",
      "iter 1770, Loss: 0.19247221946716309, Test Loss: 36.754722595214844\n",
      "iter 1771, Loss: 0.11509355902671814, Test Loss: 300.0725402832031\n",
      "iter 1772, Loss: 0.4741029739379883, Test Loss: 476.3219299316406\n",
      "iter 1773, Loss: 0.16200844943523407, Test Loss: 81.86363983154297\n",
      "iter 1774, Loss: 1.0450447797775269, Test Loss: 157.0606231689453\n",
      "iter 1775, Loss: 0.957876980304718, Test Loss: 117.5641098022461\n",
      "iter 1776, Loss: 0.3400747776031494, Test Loss: 106.75623321533203\n",
      "iter 1777, Loss: 0.1669156551361084, Test Loss: 191.5602264404297\n",
      "iter 1778, Loss: 1.0732866525650024, Test Loss: 44.82612609863281\n",
      "iter 1779, Loss: 0.6726289391517639, Test Loss: 53.51552963256836\n",
      "iter 1780, Loss: 0.3840925991535187, Test Loss: 657.2122192382812\n",
      "iter 1781, Loss: 0.8994720578193665, Test Loss: 70.39762878417969\n",
      "iter 1782, Loss: 0.113608717918396, Test Loss: 203.60791015625\n",
      "iter 1783, Loss: 0.34646371006965637, Test Loss: 60.81477737426758\n",
      "iter 1784, Loss: 0.19864927232265472, Test Loss: 89.9703369140625\n",
      "iter 1785, Loss: 0.6613621711730957, Test Loss: 89.99710845947266\n",
      "iter 1786, Loss: 0.4346555769443512, Test Loss: 83.25645446777344\n",
      "iter 1787, Loss: 0.1550709307193756, Test Loss: 331.6901550292969\n",
      "iter 1788, Loss: 0.1613858938217163, Test Loss: 66.68824005126953\n",
      "iter 1789, Loss: 0.6417548656463623, Test Loss: 381.99462890625\n",
      "iter 1790, Loss: 0.14407093822956085, Test Loss: 49.28666305541992\n",
      "iter 1791, Loss: 0.42659589648246765, Test Loss: 60.183563232421875\n",
      "iter 1792, Loss: 0.27986302971839905, Test Loss: 113.20819854736328\n",
      "iter 1793, Loss: 0.5190861821174622, Test Loss: 39.48037338256836\n",
      "iter 1794, Loss: 0.8425922989845276, Test Loss: 244.5678253173828\n",
      "iter 1795, Loss: 0.3785353899002075, Test Loss: 3196.347412109375\n",
      "iter 1796, Loss: 0.22577714920043945, Test Loss: 32.35714340209961\n",
      "iter 1797, Loss: 0.595643162727356, Test Loss: 1614.6370849609375\n",
      "iter 1798, Loss: 0.2092636376619339, Test Loss: 1407.1341552734375\n",
      "iter 1799, Loss: 0.4475042521953583, Test Loss: 1289.2618408203125\n",
      "iter 1800, Loss: 0.447345495223999, Test Loss: 38.02849197387695\n",
      "iter 1801, Loss: 0.40988942980766296, Test Loss: 197.73426818847656\n",
      "iter 1802, Loss: 0.394619345664978, Test Loss: 71.01818084716797\n",
      "iter 1803, Loss: 0.31211745738983154, Test Loss: 78.12848663330078\n",
      "iter 1804, Loss: 0.28445926308631897, Test Loss: 20.2096004486084\n",
      "iter 1805, Loss: 0.5986636281013489, Test Loss: 217.9217071533203\n",
      "iter 1806, Loss: 0.38517698645591736, Test Loss: 169.5220184326172\n",
      "iter 1807, Loss: 0.27225837111473083, Test Loss: 70.02398681640625\n",
      "iter 1808, Loss: 0.3627568781375885, Test Loss: 139.03573608398438\n",
      "iter 1809, Loss: 0.538135826587677, Test Loss: 78.34944915771484\n",
      "iter 1810, Loss: 0.21783983707427979, Test Loss: 21.635183334350586\n",
      "iter 1811, Loss: 0.3243926465511322, Test Loss: 93.70361328125\n",
      "iter 1812, Loss: 0.4972062110900879, Test Loss: 188302.421875\n",
      "iter 1813, Loss: 0.4956929683685303, Test Loss: 46.174072265625\n",
      "iter 1814, Loss: 0.4653596878051758, Test Loss: 220317.046875\n",
      "iter 1815, Loss: 0.46657487750053406, Test Loss: 15.264477729797363\n",
      "iter 1816, Loss: 0.36315464973449707, Test Loss: 180.23887634277344\n",
      "iter 1817, Loss: 0.33096492290496826, Test Loss: 167.6730194091797\n",
      "iter 1818, Loss: 0.680003821849823, Test Loss: 24.47296714782715\n",
      "iter 1819, Loss: 0.6144629716873169, Test Loss: 90.14759063720703\n",
      "iter 1820, Loss: 0.3450963497161865, Test Loss: 1952.1630859375\n",
      "iter 1821, Loss: 0.091448575258255, Test Loss: 73.46302032470703\n",
      "iter 1822, Loss: 0.6341125965118408, Test Loss: 175.29888916015625\n",
      "iter 1823, Loss: 0.04009544104337692, Test Loss: 136.915771484375\n",
      "iter 1824, Loss: 0.3751576840877533, Test Loss: 175.9826202392578\n",
      "iter 1825, Loss: 0.618598222732544, Test Loss: 81.60597229003906\n",
      "iter 1826, Loss: 0.2697751820087433, Test Loss: 10.44249439239502\n",
      "iter 1827, Loss: 0.1164683923125267, Test Loss: 98.72083282470703\n",
      "iter 1828, Loss: 0.29963287711143494, Test Loss: 9.969298362731934\n",
      "iter 1829, Loss: 0.2740647792816162, Test Loss: 108.712646484375\n",
      "iter 1830, Loss: 0.20295368134975433, Test Loss: 94.87667846679688\n",
      "iter 1831, Loss: 0.27033525705337524, Test Loss: 360.9849548339844\n",
      "iter 1832, Loss: 0.14660149812698364, Test Loss: 20192.908203125\n",
      "iter 1833, Loss: 0.6261073350906372, Test Loss: 17.728378295898438\n",
      "iter 1834, Loss: 0.5772383213043213, Test Loss: 41.88938522338867\n",
      "iter 1835, Loss: 0.3081979751586914, Test Loss: 23.371963500976562\n",
      "iter 1836, Loss: 0.1355406641960144, Test Loss: 93.00067138671875\n",
      "iter 1837, Loss: 0.2444353699684143, Test Loss: 54.184814453125\n",
      "iter 1838, Loss: 0.08042385429143906, Test Loss: 154.04774475097656\n",
      "iter 1839, Loss: 1.1162328720092773, Test Loss: 43.727569580078125\n",
      "iter 1840, Loss: 1.1755237579345703, Test Loss: 64.9424057006836\n",
      "iter 1841, Loss: 0.15173755586147308, Test Loss: 21.84832763671875\n",
      "iter 1842, Loss: 0.5920022130012512, Test Loss: 97.90802001953125\n",
      "iter 1843, Loss: 0.3899543285369873, Test Loss: 106.51416015625\n",
      "iter 1844, Loss: 0.3229031264781952, Test Loss: 51.202640533447266\n",
      "iter 1845, Loss: 0.511677086353302, Test Loss: 85.9321517944336\n",
      "iter 1846, Loss: 0.4303858280181885, Test Loss: 5135.33544921875\n",
      "iter 1847, Loss: 0.7056443095207214, Test Loss: 1198.4398193359375\n",
      "iter 1848, Loss: 0.9017489552497864, Test Loss: 1426.71484375\n",
      "iter 1849, Loss: 0.25075066089630127, Test Loss: 852.3425903320312\n",
      "iter 1850, Loss: 0.7692944407463074, Test Loss: 168.5780487060547\n",
      "iter 1851, Loss: 0.6316872239112854, Test Loss: 172.30218505859375\n",
      "iter 1852, Loss: 0.520291805267334, Test Loss: 939.5838012695312\n",
      "iter 1853, Loss: 0.6339280009269714, Test Loss: 92.6862564086914\n",
      "iter 1854, Loss: 0.4442990720272064, Test Loss: 48.57717514038086\n",
      "iter 1855, Loss: 0.1436830461025238, Test Loss: 89.78801727294922\n",
      "iter 1856, Loss: 0.4983908236026764, Test Loss: 69.70571899414062\n",
      "iter 1857, Loss: 0.7353420853614807, Test Loss: 58.47257995605469\n",
      "iter 1858, Loss: 0.36637255549430847, Test Loss: 74.5885009765625\n",
      "iter 1859, Loss: 0.20667274296283722, Test Loss: 13745873.0\n",
      "iter 1860, Loss: 0.3773078918457031, Test Loss: 10.06616497039795\n",
      "iter 1861, Loss: 0.38920894265174866, Test Loss: 183.77378845214844\n",
      "iter 1862, Loss: 0.19802401959896088, Test Loss: 46.30961990356445\n",
      "iter 1863, Loss: 0.3303664028644562, Test Loss: 87.77127075195312\n",
      "iter 1864, Loss: 0.23289704322814941, Test Loss: 43.26059341430664\n",
      "iter 1865, Loss: 0.5386416912078857, Test Loss: 439.4394836425781\n",
      "iter 1866, Loss: 0.17223797738552094, Test Loss: 64.53147888183594\n",
      "iter 1867, Loss: 0.3351577818393707, Test Loss: 73.2860107421875\n",
      "iter 1868, Loss: 0.6112065315246582, Test Loss: 41.678131103515625\n",
      "iter 1869, Loss: 0.04388673976063728, Test Loss: 7.3608832359313965\n",
      "iter 1870, Loss: 0.22403939068317413, Test Loss: 1211.8692626953125\n",
      "iter 1871, Loss: 0.2036583572626114, Test Loss: 475.9415283203125\n",
      "iter 1872, Loss: 0.17473573982715607, Test Loss: 109.23238372802734\n",
      "iter 1873, Loss: 0.3400917053222656, Test Loss: 159.3527374267578\n",
      "iter 1874, Loss: 0.18998585641384125, Test Loss: 32.00881576538086\n",
      "iter 1875, Loss: 0.37659183144569397, Test Loss: 688.5166015625\n",
      "iter 1876, Loss: 1.586363673210144, Test Loss: 250.4431610107422\n",
      "iter 1877, Loss: 0.651129424571991, Test Loss: 45.53751754760742\n",
      "iter 1878, Loss: 0.14139032363891602, Test Loss: 1548.6846923828125\n",
      "iter 1879, Loss: 0.5210941433906555, Test Loss: 196.9355010986328\n",
      "iter 1880, Loss: 0.4851793050765991, Test Loss: 383.1029052734375\n",
      "iter 1881, Loss: 1.008641004562378, Test Loss: 19.651020050048828\n",
      "iter 1882, Loss: 0.28815674781799316, Test Loss: 214.105712890625\n",
      "iter 1883, Loss: 0.41416797041893005, Test Loss: 159.3269500732422\n",
      "iter 1884, Loss: 0.6604412198066711, Test Loss: 40471.5703125\n",
      "iter 1885, Loss: 0.3499123752117157, Test Loss: 178.1647186279297\n",
      "iter 1886, Loss: 0.17174667119979858, Test Loss: 279.2411804199219\n",
      "iter 1887, Loss: 0.6098586916923523, Test Loss: 2100.696044921875\n",
      "iter 1888, Loss: 0.915172278881073, Test Loss: 4324.21630859375\n",
      "iter 1889, Loss: 0.08815749734640121, Test Loss: 3834.941162109375\n",
      "iter 1890, Loss: 0.4395996034145355, Test Loss: 226.5361785888672\n",
      "iter 1891, Loss: 0.31194186210632324, Test Loss: 53.91579818725586\n",
      "iter 1892, Loss: 0.34690332412719727, Test Loss: 27.763227462768555\n",
      "iter 1893, Loss: 0.25271233916282654, Test Loss: 2271.522705078125\n",
      "iter 1894, Loss: 0.45062923431396484, Test Loss: 41.66627883911133\n",
      "iter 1895, Loss: 0.24807916581630707, Test Loss: 244.70542907714844\n",
      "iter 1896, Loss: 0.7908720970153809, Test Loss: 22.4737491607666\n",
      "iter 1897, Loss: 0.24642451107501984, Test Loss: 72.2195816040039\n",
      "iter 1898, Loss: 0.5585262179374695, Test Loss: 102.1521224975586\n",
      "iter 1899, Loss: 0.4589921534061432, Test Loss: 11.99919605255127\n",
      "iter 1900, Loss: 0.9004599452018738, Test Loss: 168.29579162597656\n",
      "iter 1901, Loss: 0.3300042450428009, Test Loss: 73.33047485351562\n",
      "iter 1902, Loss: 0.6867103576660156, Test Loss: 48.37192153930664\n",
      "iter 1903, Loss: 0.2414674162864685, Test Loss: 78.2118911743164\n",
      "iter 1904, Loss: 0.6013438105583191, Test Loss: 19.736764907836914\n",
      "iter 1905, Loss: 0.2910849452018738, Test Loss: 1212.5423583984375\n",
      "iter 1906, Loss: 9.995312690734863, Test Loss: 3.4071176052093506\n",
      "iter 1907, Loss: 6.344447612762451, Test Loss: 372.839111328125\n",
      "iter 1908, Loss: 0.19413232803344727, Test Loss: 333.54541015625\n",
      "iter 1909, Loss: 0.4791925847530365, Test Loss: 516.4376831054688\n",
      "iter 1910, Loss: 0.49373114109039307, Test Loss: 519.3539428710938\n",
      "iter 1911, Loss: 0.12768089771270752, Test Loss: 96.1923828125\n",
      "iter 1912, Loss: 0.4973142147064209, Test Loss: 138.4617156982422\n",
      "iter 1913, Loss: 0.2868187725543976, Test Loss: 1287.6080322265625\n",
      "iter 1914, Loss: 0.5690792202949524, Test Loss: 90.87014770507812\n",
      "iter 1915, Loss: 0.3953889310359955, Test Loss: 106.27977752685547\n",
      "iter 1916, Loss: 0.5564308762550354, Test Loss: 374.5606384277344\n",
      "iter 1917, Loss: 0.17909161746501923, Test Loss: 117.77456665039062\n",
      "iter 1918, Loss: 0.31611964106559753, Test Loss: 101.3224868774414\n",
      "iter 1919, Loss: 0.5328395962715149, Test Loss: 97.08035278320312\n",
      "iter 1920, Loss: 0.43069231510162354, Test Loss: 74.38580322265625\n",
      "iter 1921, Loss: 0.8626213669776917, Test Loss: 77.3314437866211\n",
      "iter 1922, Loss: 0.1251516491174698, Test Loss: 72.12571716308594\n",
      "iter 1923, Loss: 0.2862396836280823, Test Loss: 803.176025390625\n",
      "iter 1924, Loss: 5.0740885734558105, Test Loss: 10.852371215820312\n",
      "iter 1925, Loss: 0.530423104763031, Test Loss: 55.63986587524414\n",
      "iter 1926, Loss: 0.3355503976345062, Test Loss: 16.092811584472656\n",
      "iter 1927, Loss: 0.37093386054039, Test Loss: 99.48492431640625\n",
      "iter 1928, Loss: 0.26499465107917786, Test Loss: 45.92763900756836\n",
      "iter 1929, Loss: 0.25856491923332214, Test Loss: 30.197397232055664\n",
      "iter 1930, Loss: 0.17924796044826508, Test Loss: 966.0537109375\n",
      "iter 1931, Loss: 0.4745405912399292, Test Loss: 122.17530059814453\n",
      "iter 1932, Loss: 0.08594163507223129, Test Loss: 72.9573745727539\n",
      "iter 1933, Loss: 0.32035690546035767, Test Loss: 27859600.0\n",
      "iter 1934, Loss: 0.11651894450187683, Test Loss: 69.02657318115234\n",
      "iter 1935, Loss: 0.592797040939331, Test Loss: 59.006771087646484\n",
      "iter 1936, Loss: 0.8257813453674316, Test Loss: 50.83723068237305\n",
      "iter 1937, Loss: 0.5007603764533997, Test Loss: 223.62664794921875\n",
      "iter 1938, Loss: 0.14334779977798462, Test Loss: 115.67098999023438\n",
      "iter 1939, Loss: 0.49826133251190186, Test Loss: 16.05071449279785\n",
      "iter 1940, Loss: 1.0725363492965698, Test Loss: 139.19895935058594\n",
      "iter 1941, Loss: 0.48888567090034485, Test Loss: 135.56085205078125\n",
      "iter 1942, Loss: 0.3592471778392792, Test Loss: 136.4895782470703\n",
      "iter 1943, Loss: 0.06459733843803406, Test Loss: 101.75341796875\n",
      "iter 1944, Loss: 0.5683822631835938, Test Loss: 280.9809875488281\n",
      "iter 1945, Loss: 0.36322808265686035, Test Loss: 49.09653854370117\n",
      "iter 1946, Loss: 0.03878725692629814, Test Loss: 60.80381774902344\n",
      "iter 1947, Loss: 0.06780707091093063, Test Loss: 1571.6357421875\n",
      "iter 1948, Loss: 0.2545275390148163, Test Loss: 281.3299865722656\n",
      "iter 1949, Loss: 0.46862098574638367, Test Loss: 8.925615310668945\n",
      "iter 1950, Loss: 0.3893636465072632, Test Loss: 124.49567413330078\n",
      "iter 1951, Loss: 0.3156295716762543, Test Loss: 18.534080505371094\n",
      "iter 1952, Loss: 0.10262443870306015, Test Loss: 15.28646469116211\n",
      "iter 1953, Loss: 6.710373401641846, Test Loss: 56.6451530456543\n",
      "iter 1954, Loss: 0.5866439342498779, Test Loss: 85.1255111694336\n",
      "iter 1955, Loss: 0.8500485420227051, Test Loss: 397.9181213378906\n",
      "iter 1956, Loss: 0.25523641705513, Test Loss: 107.35919189453125\n",
      "iter 1957, Loss: 0.2795545160770416, Test Loss: 31.688255310058594\n",
      "iter 1958, Loss: 0.20342695713043213, Test Loss: 114.12950897216797\n",
      "iter 1959, Loss: 0.15894146263599396, Test Loss: 60.21437454223633\n",
      "iter 1960, Loss: 0.3872392177581787, Test Loss: 43.17705154418945\n",
      "iter 1961, Loss: 0.22369205951690674, Test Loss: 53.3277702331543\n",
      "iter 1962, Loss: 0.28391802310943604, Test Loss: 300.1236267089844\n",
      "iter 1963, Loss: 0.3120369017124176, Test Loss: 545.6228637695312\n",
      "iter 1964, Loss: 0.16421055793762207, Test Loss: 44.03586196899414\n",
      "iter 1965, Loss: 0.4664587676525116, Test Loss: 114.57586669921875\n",
      "iter 1966, Loss: 0.08173616975545883, Test Loss: 75.908203125\n",
      "iter 1967, Loss: 0.3134300708770752, Test Loss: 22.02874183654785\n",
      "iter 1968, Loss: 0.532090961933136, Test Loss: 71.037841796875\n",
      "iter 1969, Loss: 0.422890305519104, Test Loss: 157.08673095703125\n",
      "iter 1970, Loss: 0.4222763478755951, Test Loss: 88.69397735595703\n",
      "iter 1971, Loss: 0.40788188576698303, Test Loss: 720.0779418945312\n",
      "iter 1972, Loss: 0.22856228053569794, Test Loss: 715.7368774414062\n",
      "iter 1973, Loss: 0.5749569535255432, Test Loss: 1552.3712158203125\n",
      "iter 1974, Loss: 0.31023311614990234, Test Loss: 28.690284729003906\n",
      "iter 1975, Loss: 0.36050650477409363, Test Loss: 964.8053588867188\n",
      "iter 1976, Loss: 0.36465397477149963, Test Loss: 162.85003662109375\n",
      "iter 1977, Loss: 0.053573232144117355, Test Loss: 110.8181381225586\n",
      "iter 1978, Loss: 0.4914593994617462, Test Loss: 10.367432594299316\n",
      "iter 1979, Loss: 0.11887023597955704, Test Loss: 138.6614532470703\n",
      "iter 1980, Loss: 0.12541605532169342, Test Loss: 404.3307189941406\n",
      "iter 1981, Loss: 0.7956117987632751, Test Loss: 9.540262222290039\n",
      "iter 1982, Loss: 0.2232527881860733, Test Loss: 157.42578125\n",
      "iter 1983, Loss: 0.25885069370269775, Test Loss: 70.05685424804688\n",
      "iter 1984, Loss: 0.29526838660240173, Test Loss: 118.20144653320312\n",
      "iter 1985, Loss: 0.7383711934089661, Test Loss: 257.9121398925781\n",
      "iter 1986, Loss: 0.09554819017648697, Test Loss: 30.57049560546875\n",
      "iter 1987, Loss: 0.004912885371595621, Test Loss: 82.52227783203125\n",
      "iter 1988, Loss: 0.25894895195961, Test Loss: 24.9169979095459\n",
      "iter 1989, Loss: 0.9994623064994812, Test Loss: 150.84596252441406\n",
      "iter 1990, Loss: 0.1290980577468872, Test Loss: 30.174346923828125\n",
      "iter 1991, Loss: 0.48346415162086487, Test Loss: 32.92555618286133\n",
      "iter 1992, Loss: 0.3413625657558441, Test Loss: 540.188720703125\n",
      "iter 1993, Loss: 0.3533405363559723, Test Loss: 132.1653289794922\n",
      "iter 1994, Loss: 0.5751398205757141, Test Loss: 48.78923034667969\n",
      "iter 1995, Loss: 0.3595554530620575, Test Loss: 294.23095703125\n",
      "iter 1996, Loss: 0.45941805839538574, Test Loss: 43.37479782104492\n",
      "iter 1997, Loss: 0.18132740259170532, Test Loss: 90.2903823852539\n",
      "iter 1998, Loss: 0.6544683575630188, Test Loss: 173.93096923828125\n",
      "iter 1999, Loss: 0.392088383436203, Test Loss: 259.7329406738281\n",
      "iter 2000, Loss: 0.5444298386573792, Test Loss: 96.8228530883789\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# Begin training!#\n",
    "##################\n",
    "\n",
    "for iter in range(num_training_iterations):\n",
    "\n",
    "  # Grab a batch and propagate it through the network\n",
    "  file_select()\n",
    "  x_batch, y_batch = get_batch(accelometer, gyroscope,speed, output_data, batch_size)\n",
    "  train_step(x_batch, y_batch)\n",
    "\n",
    "  with train_summary_writer.as_default():\n",
    "    tf.summary.scalar('loss', train_loss.result(), step=iter)\n",
    "\n",
    "  file_select_test()\n",
    "  x_batch, y_batch = get_batch(accelometer, gyroscope,speed, output_data, batch_size)\n",
    "  loss = test_step(x_batch, y_batch)\n",
    "\n",
    "  with test_summary_writer.as_default():\n",
    "    tf.summary.scalar('loss_test', test_loss.result(), step=iter)\n",
    "\n",
    "  # Update the model with the changed weights!\n",
    "  if iter % 100 == 0:     \n",
    "    model.save_weights(checkpoint_prefix)\n",
    "\n",
    "  template = 'iter {}, Loss: {}, Test Loss: {}'\n",
    "  print (template.format(iter+1,\n",
    "                         train_loss.result(), \n",
    "                         test_loss.result())) \n",
    "\n",
    "\n",
    "  # Reset metrics every epoch\n",
    "  train_loss.reset_states()\n",
    "  test_loss.reset_states()\n",
    "    \n",
    "# Save the trained model and the weights\n",
    "model.save_weights(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reusing TensorBoard on port 6006 (pid 3953), started 0:52:10 ago. (Use '!kill 3953' to kill it.)"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%tensorboard --logdir /Users/amitaflalo/Desktop/deepnav/logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}